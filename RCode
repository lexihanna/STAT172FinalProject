rm(list = ls())
# packages needed
#install.packages("yardstick")
#install.packages("caret")
library(caret)
library(yardstick)
library(ggplot2) #for graphing
library(RColorBrewer) #for custom color palettes
library(dplyr) #for manipulating, aggregating, piping

# read in data
dogTravel <- read.csv('dogTravel.csv',
                      stringsAsFactors = TRUE)
allDogDescriptions <- read.csv('allDogDescriptions.csv',
                               stringsAsFactors = TRUE)
movesByLocation <- read.csv('movesByLocation.csv',
                            stringsAsFactors = TRUE)
# check if the data was read in accurately 
str(dogTravel)
str(allDogDescriptions)
str(movesByLocation)

# clean data-----------------
## merge dogTravel and allDogDescriptions datasets
dog <- merge(dogTravel,allDogDescriptions,by="id")

## subset dog to only include necessary columns
df = subset(dog, select = c(still_there,breed_primary,
                            breed_mixed,color_primary, sex,age,size,coat,fixed,
                            special_needs,shots_current,env_children,
                            env_dogs,env_cats,house_trained, description.x, id
))

# changing the variable name of description.x to be more readable
df %>% rename(description = description.x)
# changing description to a character column for LDA
df$description <- as.character(df$description)

## look for missing values
# so that our summary and structure calls do not get muddled by the description
cols = c("still_there","breed_primary",
         "breed_mixed","color_primary", "sex","age","size","coat","fixed",
         "special_needs","shots_current","env_children",
         "env_dogs","env_cats","house_trained", "id")
summary(df[cols])
# missing values are in still_there, color_primary,
# coat, env_children, env_dogs, and env_cats
# let's clean these variables

# cleaning still_there
table(df$still_there)
# changing still_there from a factor to character so we can modify it
df$still_there <- as.character(df$still_there)
# changing the missing values to False
df$still_there[df$still_there ==""] <- "False"
# check to see if it worked
table(df$still_there)
# changing still_there back to a factor
df$still_there <- as.factor(df$still_there)
# check to see if it worked
str(df$still_there)

# cleaning coat
table(df$coat)
# changing coat from a factor to character so we can modify it
df$coat <- as.character(df$coat)
# changing the missing values to Mixed
df$coat[df$coat ==""] <- "Mixed"
# check to see if it worked
table(df$coat)
#changing Curly, Hairless, Wire to Curly / Hairless / Wire
df$coat[df$coat %in% c("Curly", "Hairless", "Wire")] <- "Curly / Hairless / Wire"
# check to see if it worked
table(df$coat)
# sort the variables
df$coat <- factor(df$coat,
                  levels = c("Short", "Medium", "Long", "Mixed", "Curly / Hairless / Wire"))
# check to see if it worked
table(df$coat)
str(df$coat)

# cleaning env_children
table(df$env_children)
# changing env_children from a factor to character so we can modify it
df$env_children <- as.character(df$env_children)
# replacing the missing values with the mode of the column
df$env_children[df$env_children ==""] <- "True"
# check to see if it worked
table(df$env_children)
# changing env_children back to a factor
df$env_children <- as.factor(df$env_children)
# check to see if it worked
str(df$env_children)

# cleaning env_dogs
table(df$env_dogs)
# changing env_dogs from a factor to character so we can modify it
df$env_dogs <- as.character(df$env_dogs)
# replacing the missing values with the mode of the column
df$env_dogs[df$env_dogs ==""] <- "True"
# check to see if it worked
table(df$env_dogs)
# changing env_dogs back to a factor
df$env_dogs <- as.factor(df$env_dogs)
# check to see if it worked
str(df$env_dogs)

# cleaning env_cats
table(df$env_cats)
# changing env_cats from a factor to character so we can modify it
df$env_cats <- as.character(df$env_cats)
# replacing the missing values with the mode of the column
df$env_cats[df$env_cats ==""] <- "True"
# check to see if it worked
table(df$env_cats)
# changing env_cats back to a factor
df$env_cats <- as.factor(df$env_cats)
# check to see if it worked
str(df$env_cats)

#let's look at what we've changed
str(df[cols])
summary(df[cols])

# let's assign dummy variables to breed_primary
# we want to look at the top 10 breeds and assign the other breeds as "other"
sort(table(df$breed_primary), decreasing = TRUE)[1:10]
`%notin%` <- Negate(`%in%`)
# changing breed_primary from a factor to character so we can modify it
df$breed_primary <- as.character(df$breed_primary)
df$breed_primary[df$breed_primary %notin% c("Labrador Retriever", "Chihuahua", "Pit Bull Terrier",
                                            "German Shepherd Dog", "Mixed Breed", "Hound", "Shepherd",
                                            "Terrier", "Beagle", "Boxer")] <- "Other"
# check if it worked 
table(df$breed_primary)
# convert back to a factor 
df$breed_primary <- as.factor(df$breed_primary)

#let's look at color_primary
summary(df$color_primary)
# let's assign dummy variables to color_primary
# we want to look at the color_primary categories distribution to combine the smallest categories
sort(table(df$color_primary), decreasing = TRUE)
# changing breed_primary from a factor to character so we can modify it
df$color_primary <- as.character(df$color_primary)
df$color_primary[df$color_primary %in% c("Merle (Blue)", "Sable", "Merle (Red)", 
                                         "Harlequin")] <- "Merle / Sable / Harlequin"
df$color_primary[df$color_primary %in% c("")] <- "Unknown"
# let's check!
df$color_primary <- as.factor(df$color_primary)
table(df$color_primary)

# cleaning the sex variable
summary(df$sex)
# the variable has 3 categories but the category 'Unknown' has 0 observations
# change sex form a factor to a character so we can modify it 
df$sex <- as.character(df$sex)
# so, we just remove that level 
df$sex[df$sex %in% c("Male", "Unknown")] <- "Male"
# let's check!
df$sex <- as.factor(df$sex)
table(df$sex)

# let's see our final dataset
summary(df[cols])

# complete separation---------------
## Response Variable 
table(df$still_there)
# So, around 95% of the dogs from these data were adopted (False, 0) and 5%
# of them were not adopted (True, 1).
## Exploratory Variable
### Categorical Variables (checking for complete separation)

#### distribution table for the breed_primary variable and still_there
freq <- prop.table(table(df$still_there, df$breed_primary),2)*100
freq
#### No complete separation for this variable 

#### distribution table for the breed_mixed variable and still_there
freq <- prop.table(table(df$still_there, df$breed_mixed),2)*100
freq 
#### No complete separation for this variable 

#### distribution table for the color_primary variable and still_there
dist <- prop.table(table(df$still_there, df$color_primary),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the sex variable and still_there
dist <- prop.table(table(df$still_there, df$sex),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the age variable and still_there
dist <- prop.table(table(df$still_there, df$age),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the size variable and still_there
dist <- prop.table(table(df$still_there, df$size),2)*100
dist 
#### No complete separation for this variable 


#### distribution table for the coat variable and still_there
dist <- prop.table(table(df$still_there, df$coat),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the fixed variable and still_there
dist <- prop.table(table(df$still_there, df$fixed),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the special_needs variable and still_there
dist <- prop.table(table(df$still_there, df$special_needs),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the shots_current variable and still_there
dist <- prop.table(table(df$shots_current, df$fixed),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the env_children variable and still_there
dist <- prop.table(table(df$still_there, df$env_children),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the env_dogs variable and still_there
dist <- prop.table(table(df$still_there, df$env_dogs),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the env_cats variable and still_there
dist <- prop.table(table(df$still_there, df$env_cats),2)*100
dist 
#### No complete separation for this variable 

#### distribution table for the house_trained variable and still_there
dist <- prop.table(table(df$still_there, df$house_trained),2)*100
dist 
#### No complete separation for this variable

# visualizations---------------
# predictive model--------------
# creating a binary version of still_there to use in Bernoulli and Random Forests
table(df$still_there)
df <- df %>% 
  mutate(still_there_bin = ifelse(still_there == "True", 1, 0))
# checking to see if it worked
summary(df$still_there_bin)
str(df[cols])


# changing still_there_bin to a factor to run a random forest for variable selection
df$still_there_bin <- as.factor(df$still_there_bin)
# creating a variable selection forest to create a bernoulli model
bern_var_selection <- randomForest(still_there_bin~ breed_primary + breed_mixed + 
                                    color_primary + sex + age + size + coat + fixed + 
                                    special_needs + shots_current + env_children + env_dogs + 
                                    env_cats + house_trained,
                                  data= df, # all of the data
                                  ntree = 1000, #B = # of classification trees in forest
                                  mtry = 4, # choose m = sqrt(14) = 3.74 approx
                                  importance = TRUE)
# variable importance plot!
vi <- as.data.frame(varImpPlot(bern_var_selection, type = 1))
# prettier plot! 
vi$Variable <- rownames(vi)
ggplot(data = vi) +
  geom_bar(aes(x = reorder(Variable, MeanDecreaseAccuracy), weight = MeanDecreaseAccuracy),
           position = "identity") +
  coord_flip() +
  labs( x = "Variable Name", y = "Importance") + 
  ggtitle("Bernoulli Model Variable Importance Plot") +
  theme(plot.title = element_text(hjust = 0.5))

# splitting into training and testing splits to be able to compare our models after lda
RNGkind(sample.kind = "default")
set.seed(2291352)
train.idx <- sample(x = 1:nrow(df), size = .7*nrow(df))
train.df <- df[train.idx, ]
test.df <- df[-train.idx, ]

# building a Bernoulli model based off of the variable selection plot
# ROUND 1: color_primary + breed_primary + coat
m1 <- glm(still_there ~ color_primary + breed_primary + coat, 
          data = train.df,
          family = binomial(link = "logit"))
AIC(m1) # 1705.233
BIC(m1) # 1877.344
# ROUND 2: color_primary + breed_primary + coat + env_children + house_trained
m2 <- glm(still_there ~ color_primary + breed_primary + coat + env_children + house_trained, 
          data = train.df,
          family = binomial(link = "logit"))
AIC(m2) # 1677.779 better, let's keep going!
BIC(m2) # 1862.639 better, let's keep going!
# ROUND 3: color_primary + breed_primary + coat + env_children + house_trained + size
m3 <- glm(still_there ~ color_primary + breed_primary + coat + env_children + house_trained + 
            size, 
          data = train.df,
          family = binomial(link = "logit"))
AIC(m3) # 1677.755 better, but barely
BIC(m3) # 1881.738 worse, let's stop at the previous model.
# final bernoulli model without any additions:
final_bern_model <- glm(still_there ~ color_primary + breed_primary + coat + env_children + 
                          house_trained, 
                          data = train.df,
                          family = binomial(link = "logit"))
summary(final_bern_model)
# AIC: 1677.779
# BIC: 1862.639

# let's fine tune pi*
pi_hat <- predict(final_bern_model, test.df, type = "response")
rocCurve <- roc(response = test.df$still_there_bin,
                predictor = pi_hat,
                levels = c("0", "1"))
plot(rocCurve, print.thres = TRUE, print.auc = TRUE)
# PI STAR: 0.054
# SENSITIVITY: 0.703
# SPECIFICITY: 0.703
# AUC: 0.754
pi_star <- coords(rocCurve, "best", ret = "threshold")$threshold[1]
test.df$bern_pred <- as.factor(ifelse(pi_hat > pi_star, "1", "0"))
pi_star

# Confusion Matrix for our final Bernoulli model with Random Forest Variable Selection
test.df$still_there_bin <- as.factor(test.df$still_there_bin)
cm <- conf_mat(test.df, still_there_bin, bern_pred)
autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1") +
  labs( x = "Truth", y = "Bernoulli Model Prediction") + 
  ggtitle("Bernoulli Model Confusion Matrix") +
  theme(plot.title = element_text(hjust = 0.5))
# descriptive model---------------
# let's first fine tune our m parameter
mtry <- c(1:14)
keeps <- data.frame(m = rep(NA, length(mtry)),
                    OOB_error_rate = rep(NA, length(mtry)))
for(idx in 1:length(mtry)){
  print(paste0("Fitting m =", mtry[idx]))
  tempforest <- randomForest(still_there_bin ~ breed_primary + breed_mixed + 
                               color_primary + sex + age + size + coat + fixed + 
                               special_needs + shots_current + env_children + env_dogs + 
                               env_cats + house_trained,
                             data = train.df,
                             ntree = 1000,
                             mtry = mtry[idx])
  print(paste0(mean(predict(tempforest) != train.df$still_there_bin)))
  keeps[idx, "m"] <- mtry[idx]
  keeps[idx, "OOB_error_rate"] <- mean(predict(tempforest) != train.df$still_there_bin)
}
# plot it!
ggplot(data = keeps) +
  geom_line(aes(x = m, y = OOB_error_rate)) +
  theme_bw() + labs( x = "m (mtry) value", y = "OOB error rate") +
  scale_x_continuous(breaks = c(1:14)) + 
  ggtitle("Random Forest: Fine Tuning m") +
  theme(plot.title = element_text(hjust = 0.5))
# the best oob error rate is when m = 4, so let's use it!
finalforest <- randomForest(still_there_bin ~ breed_primary + breed_mixed + 
                             color_primary + sex + age + size + coat + fixed + 
                             special_needs + shots_current + env_children + env_dogs + 
                             env_cats + house_trained,
                           data = train.df,
                           ntree = 1000,
                           mtry = 4)

# let's tune pi*!
pi_hat <- predict(finalforest, test.df, type = "prob")[, "1"] # extract prob of positive event, 
rocCurve <- roc(response = test.df$still_there_bin,
                predictor = pi_hat,
                levels = c("0", "1"))
plot(rocCurve, print.thres = TRUE, print.auc = TRUE)
# PI STAR: 0.10
# SENSITIVITY: 0.920
# SPECIFICITY: 0.535
# AUC: 0.754
pi_star <- coords(rocCurve, "best", ret = "threshold")$threshold[1]
test.df$forest_pred <- as.factor(ifelse(pi_hat > pi_star, "1", "0"))
pi_star

# Confusion Matrix for our final LDA model with Random Forest Variable Selection
test.df$still_there_bin <- as.factor(test.df$still_there_bin)
cm <- conf_mat(test.df, still_there_bin, forest_pred)
autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1") +
  labs( x = "Truth", y = "Random Forest Prediction") + 
  ggtitle("Random Forest Confusion Matrix") +
  theme(plot.title = element_text(hjust = 0.5))

# data mining enhancement--------------
#install.packages("tidytext")
#install.packages("topicmodels")
#install.packages("reshape2")
library(tidytext) #for (tidy) text mining
library(topicmodels) #for LDA (topic modeling)
library(reshape2) #for reshaping data (long to wide or wide to long)
library(randomForest)
library(pROC)

# I'm creating a new, tokenized data frame that is called "tokens"
tokens <- df %>% unnest_tokens(word, description)
# see first few rows - note reach row is now a single word (token)
head(tokens)

# load stop_words data frame
data(stop_words)
# remove all rows consisting of a stop word
tokens_clean <- tokens %>% anti_join(stop_words)
# seeing what it looks like
head(tokens_clean)

# counting the important words
# note that id is like a statement ID
tokens_count <- tokens_clean %>%
  # include id so it counts within unique id
  count(id, word, sort = TRUE)%>%
  ungroup()
#t o do LDA, we need what is called a "Document Term Matrix" or DTM
dtm <- tokens_count %>%
  cast_dtm(id, word, n)

# now we can perform LDA! need to specify the number of topics (k)
lda <- LDA(dtm, k = 20, control = list(seed = 1234))
# a higher beta means that word is important for that topic
topics <- tidy(lda, matrix = "beta")
# get a small data frame of the top 10 words for each topic
top_terms <- topics %>%
  group_by(topic) %>% #within each topic do some function
  top_n(10, beta) %>% #that function is take the top 10 in terms of beta
  ungroup() %>%
  arrange(topic, -beta) #order (just for the plot)
top_terms

top_terms %>%
  mutate(term = reorder(term, beta)) %>% #reorder term/word for plot
  ggplot() +
  geom_col(aes(x=term,y= beta, fill = factor(topic)),show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() #make words easier to read

#per-document-per-topic probabilities
documents <- tidy(lda, matrix = "gamma")
documents_w<- documents %>%
  select(document, topic, gamma) %>%
  dcast(document ~ topic, value.var = "gamma")
colnames(documents_w) <- c("id", paste0("Topic", c(1:20)))
df_lda <- merge(documents_w, df, by="id", all = T)
str(df_lda)

#some dogs didn't have last descriptions... we can remove them for this analysis
df_lda <- subset(df_lda, !is.na(Topic1))
#model probability of adoption based on topic probabilities
#Y = 1 if dog is still there; 0 otherwise
#Y ~ Bernoulli(p)
#logit(p) = beta_0 + beta_1*topic1 + beta2*topic2 + ...

# creating a variable selection forest to create a bernoulli model with lda
lda_var_selection <- randomForest(still_there_bin~Topic1 + Topic2 + Topic3 + Topic4 + Topic5 + Topic6 + 
                                    Topic7 + Topic8 + Topic9 + Topic10 + Topic11 + Topic12 + 
                                    Topic13 + Topic14 + Topic15 + Topic16 + Topic17 + Topic18 + 
                                    Topic19 + Topic20 + breed_primary + breed_mixed + 
                                    color_primary + sex + age + size + coat + fixed + 
                                    special_needs + shots_current + env_children + env_dogs + 
                                    env_cats + house_trained, # remember dangers here
                                  data= df_lda, # TRAINING DATA
                                  ntree = 1000, #B = # of classification trees in forest
                                  mtry = 4, # choose m = sqrt(34) = 5.831 approx
                                  importance = TRUE)
# variable importance plot!
vi <- as.data.frame(varImpPlot(lda_var_selection, type = 1))
# a prettier plot!
vi$Variable <- rownames(vi)
ggplot(data = vi) +
  geom_bar(aes(x = reorder(Variable, MeanDecreaseAccuracy), weight = MeanDecreaseAccuracy),
           position = "identity") +
  coord_flip() +
  labs( x = "Variable Name", y = "Importance") + 
  ggtitle("LDA Bernoulli Model Variable Importance Plot") +
  theme(plot.title = element_text(hjust = 0.5))
# splitting into training and testing splits to be able to compare our models before lda
RNGkind(sample.kind = "default")
set.seed(2291352)
train1.idx <- sample(x = 1:nrow(df_lda), size = .7*nrow(df_lda))
train1.df <- df_lda[train1.idx, ]
test1.df <- df_lda[-train1.idx, ]

# building a bernoulli model based off of the variable selection plot
# seeing what data type we are working with
str(train1.df$still_there_bin)
# changing to numeric
train1.df$still_there_bin <- as.numeric(train1.df$still_there_bin)
# checking to see what changed
summary(train1.df$still_there_bin)
# adjusting to be between 0 and 1
train1.df$still_there_bin <- train1.df$still_there_bin - 1
# checking to see if it worked
summary(train1.df$still_there_bin)
# ROUND 1: color_primary + breed_primary + coat + Topic19
m1 <- glm(still_there_bin ~ color_primary + breed_primary + coat + Topic19, 
          data = train1.df,
          family = binomial(link = "logit"))
AIC(m1) # 1555.442
BIC(m1) # 1733.928
# ROUND 2: color_primary + breed_primary + coat + Topic19 + Topic7 + house_trained
m2 <- glm(still_there_bin ~ color_primary + breed_primary + coat + Topic19 + Topic7 + 
            house_trained, 
          data = train1.df,
          family = binomial(link = "logit"))
AIC(m2) # 1485.237 better! keep going
BIC(m2) # 1676.471 better! keep going
# ROUND 3: color_primary + breed_primary + coat + Topic19 + Topic7 + house_trained + Topic18 +
# Topic15 + Topic14
m3 <- glm(still_there_bin ~ color_primary + breed_primary + coat + Topic19 + Topic7 + 
            house_trained + Topic18 + Topic15 + Topic14, 
          data = train1.df,
          family = binomial(link = "logit"))
AIC(m3) # 1416.095 better! keep going
BIC(m3) # 1626.453 better! keep going
# ROUND 4: color_primary + breed_primary + coat + Topic19 + Topic7 + house_trained + Topic18 +
# Topic15 + Topic14 + Topic16 + Topic2
m4 <- glm(still_there_bin ~ color_primary + breed_primary + coat + Topic19 + Topic7 + 
            house_trained + Topic18 + Topic15 + Topic14 + Topic16 + Topic2, 
          data = train1.df,
          family = binomial(link = "logit"))
AIC(m4) # 1403.492 better! keep going
BIC(m4) # 1626.493 worse, but AIC got better. Let's keep going and see if we can improve further.
# ROUND 5: color_primary + breed_primary + coat + Topic19 + Topic7 + house_trained + Topic18 +
# Topic15 + Topic14 + Topic16 + Topic2 + Topic8
m5 <- glm(still_there_bin ~ color_primary + breed_primary + coat + Topic19 + Topic7 + 
            house_trained + Topic18 + Topic15 + Topic14 + Topic16 + Topic2 + Topic8, 
          data = train1.df,
          family = binomial(link = "logit"))
AIC(m5) # 1405.492 worse, stop!
BIC(m5) # 1634.974 worse, stop!

# Final LDA model with Random Forest Variable Selection:
final_lda_model <- glm(still_there_bin ~ color_primary + breed_primary + coat + Topic19 + Topic7 + 
                         house_trained + Topic18 + Topic15 + Topic14 + Topic16 + Topic2, 
                       data = train1.df,
                       family = binomial(link = "logit"))
summary(final_lda_model)

# let's fine tune pi*
pi_hat <- predict(final_lda_model, test1.df, type = "response")
rocCurve <- roc(response = test1.df$still_there_bin,
                predictor = pi_hat,
                levels = c("0", "1"))
plot(rocCurve, print.thres = TRUE, print.auc = TRUE)
# PI STAR: 0.038
# SENSITIVITY: 0.708
# SPECIFICITY: 0.812
# AUC: 0.819
pi_star <- coords(rocCurve, "best", ret = "threshold")$threshold[1]
test1.df$lda_pred <- as.factor(ifelse(pi_hat > pi_star, "1", "0"))
pi_star

# Confusion Matrix for our final LDA model with Random Forest Variable Selection
test1.df$still_there_bin <- as.factor(test1.df$still_there_bin)
cm <- conf_mat(test1.df, still_there_bin, lda_pred)
autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1") +
  labs( x = "Truth", y = "LDA Model Prediction") + 
  ggtitle("LDA Model Confusion Matrix") +
  theme(plot.title = element_text(hjust = 0.5))
